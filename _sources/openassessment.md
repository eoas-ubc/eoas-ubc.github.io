# Open source, automated assessments

## The challenge

Three questions to be addressed:

1. Are there opensource facilities and efficient workflows for generating "randomized" questions to be used by students for practice and assessment purposes?
2. Can such facilities be sufficiently reliable, secure, consistent and easy to use so that instructors will incorporate them into their teaching practices with a minimum of time, energy and overhead?
3. To what extent can assessment of students' use of questions be automated?
4. What strategies can be used to increase the amount and effectiveness of feedback to students so that they learn from the assessments they complete?

## Solutions

To be discussed: several pathways or workflows that enable constructing questions, randomizing them, deploying, assessing results, automating grading, and adding feedback for students.

1. Sending questions built offline to Canvas
   1. Benjamin Chang's 2020-2021 work as a work-learn student on projects called md2canvas, 340quizzes and canvas-api ([BC's repo](https://github.com/bkchang-97/eoas-wl)).
   2. Procedure for efficient use of the open source (third party) [text2QTI application](tools.md).
2. nbgrader
3. PrairieLearn
4. gradscope
5. Berkeley's [OK](https://okpy.org/)
6. LMS (Canvas)
7. Berkeley (again) [OtterGrader](https://otter-grader.readthedocs.io/en/latest/)
8. Microsoft's [PyBryt](https://microsoft.github.io/pybryt/html/index.html)

Some notes on pros, cons, costs, benefits, and security, including consideration of sustainability and maintenance requirements.
